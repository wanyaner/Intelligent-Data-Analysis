### Cross Entropy

[wiki](https://www.wikiwand.com/en/Cross_entropy)  

In **information theory**, the `cross entropy` between two `probability distributions` p and q over the same underlying set of events **measures the average number of bits needed to identify an event drawn from the set**, if a coding scheme is used that is optimized for an "unnatural" probability distribution q, rather than the "true" distribution p.

在信息论中，基于相同事件测度的两个概率分布p和q的交叉熵是指，当基于一个“非自然”（相对于“真实”分布p而言）的概率分布q进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。

Cross Entropy equation:  

![equ](https://wikimedia.org/api/rest_v1/media/math/render/svg/80bd13c723dce5056a6f3aa1b29e279fb90d40bd)  

